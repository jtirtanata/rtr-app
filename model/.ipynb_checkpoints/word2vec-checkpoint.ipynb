{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vec\n",
    "### Goals \n",
    "1. Find negative reviews \n",
    "2. Grab the negative topics for each dress \n",
    "3. Extension: recommend size to pick out\n",
    "\n",
    "## Table of content \n",
    "- [fetch reviews from mongo](#mongo)\n",
    "- [preprocessing](#preprocessing)\n",
    "- [word2vec model](#model)\n",
    "- [word2vec model with bigrams](#bigram)\n",
    "\n",
    "## Negative topics\n",
    "- [get negative words](#negWords)\n",
    "- [fetch negative sentences](#negSentences)\n",
    "- [topic analysis on negative sentences](#topicModelling)\n",
    "- [negative topics for body types](#negBt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch reviews from Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from pymongo import MongoClient\n",
    "import nltk\n",
    "import gensim\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('ec2-34-198-179-91.compute-1.amazonaws.com', 27017)\n",
    "db = client.fletcher\n",
    "rev_col = db.rtr_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rev = pd.DataFrame(list(rev_col.find({}, {'review': 1, '_id': 0} )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dress = pd.DataFrame(list(rev_col.find({}, {'url': 1, '_id': 0} )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sw.remove('too')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = df_rev.review.str.lower()\n",
    "tokens = tokens.str.replace('-', ' ')\n",
    "tokens = tokens.str.replace('.', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = tokens.apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in document if word not in sw and word[0] not in string.punctuation]\n",
    "         for document in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### Word 2 Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(texts, size=100, window=5, min_count=1, workers=2,sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bigram\"> </a>\n",
    "### Word 2 Vec with bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_transformer = gensim.models.Phrases(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephinetirtanata/anaconda/lib/python3.5/site-packages/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigram_model = gensim.models.Word2Vec(bigram_transformer[texts], size=100, window=5, min_count=1, workers=2,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model.save(open('../data/bigram_word2vec.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Topics Analysis\n",
    "<a id=\"negWords\"></a>\n",
    "### Fetching negative words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_words = ['problem', 'con', 'issue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "problem_words = set()\n",
    "for word in negative_words:\n",
    "    problem_words = problem_words.union(set([word[0] for word in model.most_similar(word, topn=10)]))\n",
    "problem_words = problem_words.union(set(negative_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adjusted',\n",
       " 'aspect',\n",
       " 'besides',\n",
       " 'complaint',\n",
       " 'complaints',\n",
       " 'con',\n",
       " 'concern',\n",
       " 'concerns',\n",
       " 'doable',\n",
       " 'downfall',\n",
       " 'downside',\n",
       " 'drawback',\n",
       " 'flaw',\n",
       " 'issue',\n",
       " 'issues',\n",
       " 'manageable',\n",
       " 'picky',\n",
       " 'problem',\n",
       " 'problematic',\n",
       " 'problems',\n",
       " 'ridiculously',\n",
       " 'stubborn',\n",
       " 'thats',\n",
       " 'tolerable',\n",
       " 'trouble'}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"negSentences\"></a>\n",
    "## Filter out the negative sentences \n",
    "- Count vectorizer \n",
    "- And then tfidf - svg to find the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = df_rev.review.apply(nltk.sent_tokenize)\n",
    "df_sent = pd.concat([pd.DataFrame({'review': x, 'index': i}) for i,x in enumerate(sentences)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sent.review = df_sent.review.str.replace('.', ' ')\n",
    "df_sent.review = df_sent.review.str.replace(',', ' ')\n",
    "df_sent.review = df_sent.review.str.replace('-', ' ')\n",
    "df_sent.review = df_sent.review.str.replace('!', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove negative words that were negated\n",
    "for word in ['issue', 'problem', 'complaint', 'trouble']:\n",
    "    df_sent.review = df_sent.review.str.replace('no ' + word, ' ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put the problem words as CountVectorizer's vocabulary\n",
    "cv = CountVectorizer(vocabulary=problem_words)\n",
    "count_vector = cv.fit_transform(df_sent.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_problem = pd.DataFrame(count_vector.toarray(), columns= cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine all of the problem counts\n",
    "df_problem['sum'] = df_problem.sum(axis=1)\n",
    "df_sent['sum'] = df_problem['sum']\n",
    "df_problem_sent = df_sent[df_sent['sum'] > 0]\n",
    "df_problem_sent = df_problem_sent.join(df_dress, on='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topicModelling\"></a>\n",
    "## Topic modelling on negative sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add the problem words to the set, so that it doesn't mess up with the topic modelling\n",
    "problem_sw = set(sw).union(set(problem_words))\n",
    "problem_sw = problem_sw.union({'dress'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=problem_sw, ngram_range=(1,2))\n",
    "tfidf = tfidf_vectorizer.fit_transform(df_problem_sent.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print ([word[0] for word in (sorted(list(zip(feature_names, topic)), key=lambda x: x[1], reverse=True)[:n])])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=20, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5)\n",
    "topic_vector = nmf.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "['length', 'biggest length', 'biggest', 'perfect', 'heels length', 'length heels', 'short length', 'heels', 'length long', 'gown', 'length perfect', 'regular length', 'think length', 'size length', 'length too', 'one length', 'minor length', 'would', 'regular', 'would length']\n",
      "Topic #1:\n",
      "['zipper', 'zipper little', 'sticky', 'get', 'tough', 'sticky zipper', 'stuck', 'also zipper', 'mentioned', 'also', 'zipper tough', 'getting', 'getting zipper', 'zipper worked', 'side', 'way', 'lot zipper', 'otherwise', 'got', 'zipped']\n",
      "Topic #2:\n",
      "['long', 'too long', 'too', 'little long', 'sleeves', 'long enough', 'bit long', 'bit too', 'even', 'way too', 'long frame', 'frame', 'way', 'enough', 'tad', 'straps', 'heels', 'quite long', 'perfect', 'sleeves long']\n",
      "Topic #3:\n",
      "['sequins', 'arm', 'scratchy', 'itchy', 'sequins arms', 'underarms', 'sequins little', 'rub', 'skin', 'uncomfortable', 'scratching', 'sequins rub', 'sequins rubbing', 'hair', 'itchy sequins', 'rubbing', 'would', 'little scratchy', 'sequins scratched', 'mentioned']\n",
      "Topic #4:\n",
      "['bra', 'back', 'wear bra', 'low', 'strapless', 'strapless bra', 'low back', 'back low', 'regular bra', 'wore', 'normal bra', 'without bra', 'normal', 'open', 'open back', 'bra back', 'regular', 'show', 'back bra', 'lower']\n",
      "Topic #5:\n",
      "['short', 'too short', 'too', 'little too', 'little short', 'bit short', 'pretty short', 'short heels', 'short front', 'pretty', 'short length', 'length too', 'heels too', 'heels', 'too much', 'front', 'tall', 'much', 'short riding', 'regular']\n",
      "Topic #6:\n",
      "['wear', 'able wear', 'wear flats', 'flats', 'able', 'wear heels', 'regular', 'wear regular', 'wear bra', 'heels', 'could', 'without', 'regular bra', 'could wear', 'normally', 'wear without', 'enough', 'spanx', 'enough wear', 'still']\n",
      "Topic #7:\n",
      "['fit', 'great', 'well', 'fit well', 'perfectly', 'fit perfectly', 'fit great', 'body', 'areas', 'great fit', 'body fit', 'one fit', 'perfect', 'like', 'fit like', 'glove', 'like glove', 'fit perfect', 'one', 'dresses fit']\n",
      "Topic #8:\n",
      "['getting', 'kept', 'kept getting', 'hair', 'caught', 'getting caught', 'stuck', 'hair kept', 'getting stuck', 'caught sequins', 'wore hair', 'stuck sequins', 'zipper kept', 'lace', 'wore', 'sequins getting', 'back', 'get', 'get caught', 'night']\n",
      "Topic #9:\n",
      "['little', 'zipper little', 'little long', 'little tight', 'little bit', 'little too', 'tight', 'arms little', 'little scratchy', 'little short', 'little big', 'sleeves little', 'little itchy', 'sequins little', 'big', 'around', 'scratchy', 'straps little', 'itchy', 'loose']\n",
      "Topic #10:\n",
      "['without', 'able', 'dance', 'able dance', 'dance night', 'night', 'night without', 'comfortable able', 'away without', 'night away', 'away', 'move', 'able wear', 'able eat', 'danced', 'however able', 'eat', 'sit', 'move around', 'without bra']\n",
      "Topic #11:\n",
      "['heels', 'inch', 'inch heels', 'wore', 'wore inch', 'even', 'even inch', 'still', 'heels length', 'wore heels', 'heels still', 'heels too', 'long even', 'length inch', 'heels without', 'regular', 'tall', 'wear heels', 'ground', 'length heels']\n",
      "Topic #12:\n",
      "['arms', 'rubbing', 'rubbing arms', 'sequins arms', 'sequins rubbing', 'arms little', 'scratched', 'rubbed', 'scratchy arms', 'rubbing sequins', 'tight', 'itchy arms', 'dancing', 'scratched arms', 'scratch', 'rubbed arms', 'sequence', 'sleeves', 'scratch arms', 'arms bit']\n",
      "Topic #13:\n",
      "['bit', 'little bit', 'bit long', 'bit short', 'bit too', 'bit tight', 'bit big', 'tad bit', 'tiny bit', 'tiny', 'arms bit', 'tight', 'zipper bit', 'sleeves bit', 'scratched', 'tad', 'enough', 'sequins scratched', 'big', 'sleeves']\n",
      "Topic #14:\n",
      "['size', 'true', 'true size', 'would', 'fit true', 'smaller', 'ordered', 'waist', 'larger', 'size zipper', 'size 10', '10', 'smaller size', 'one size', 'though', 'size would', 'normal size', 'size ordered', 'think', 'backup']\n",
      "Topic #15:\n",
      "['really', 'really length', 'really even', 'zipper really', 'really great', 'really difficult', 'really big', 'sequins really', 'even', 'great', 'would', 'though', 'night', 'really stretched', 'cut really', 'really comfortable', 'one', 'great really', 'material', 'run really']\n",
      "Topic #16:\n",
      "['comfortable', 'flattering', 'comfortable flattering', 'super comfortable', 'super', 'comfortable sequins', 'comfortable straps', 'also comfortable', 'felt comfortable', 'extremely comfortable', 'felt', 'also', 'extremely', 'comfortable long', 'surprisingly', 'really comfortable', 'surprisingly comfortable', 'straps', 'sleeves', 'comfortable able']\n",
      "Topic #17:\n",
      "['area', 'bust', 'chest', 'top', 'big', 'small', 'chest area', 'too', 'too big', 'tight', 'bust area', 'large', 'one', 'big chest', 'little big', 'small bust', 'tummy area', 'small chest', 'tummy', 'bit big']\n",
      "Topic #18:\n",
      "['low', 'cut', 'low cut', 'little low', 'front', 'pin', 'safety', 'cut front', 'safety pin', 'cut chest', 'chest', 'back low', 'cut really', 'neck', 'pretty low', 'taste', 'fixed', 'cleavage', 'cut low', 'front cut']\n",
      "Topic #19:\n",
      "['zip', 'difficult', 'hard', 'difficult zip', 'hard zip', 'zipper hard', 'zipper difficult', 'little hard', 'get', 'extremely', 'extremely difficult', 'hard get', 'zipper extremely', 'small zipper', 'little difficult', 'fabric', 'zipper', 'small', 'zipper little', 'zip fit']\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf, tfidf_vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic #1:\n",
    "['zipper', 'zipper little', 'sticky', 'get', 'tough', 'sticky zipper', 'stuck', 'also zipper', 'mentioned', 'also', 'zipper tough', 'getting', 'getting zipper', 'zipper worked', 'side', 'way', 'lot zipper', 'otherwise', 'got', 'zipped']\n",
    "- Topic #2:\n",
    "['long', 'too long', 'too', 'little long', 'sleeves', 'long enough', 'bit long', 'bit too', 'even', 'way too', 'long frame', 'frame', 'way', 'enough', 'tad', 'straps', 'heels', 'quite long', 'perfect', 'sleeves long']\n",
    "- Topic #4:\n",
    "['bra', 'back', 'wear bra', 'low', 'strapless', 'strapless bra', 'low back', 'back low', 'regular bra', 'wore', 'normal bra', 'without bra', 'normal', 'open', 'open back', 'bra back', 'regular', 'show', 'back bra', 'lower']\n",
    "- Topic #5:\n",
    "['short', 'too short', 'too', 'little too', 'little short', 'bit short', 'pretty short', 'short heels', 'short front', 'pretty', 'short length', 'length too', 'heels too', 'heels', 'too much', 'front', 'tall', 'much', 'short riding', 'regular']', 'night', 'really stretched', 'cut really', 'really comfortable', 'one', 'great really', 'material', 'run really']\n",
    "- Topic #8:\n",
    "['getting', 'kept', 'kept getting', 'hair', 'caught', 'getting caught', 'stuck', 'hair kept', 'getting stuck', 'caught sequins', 'wore hair', 'stuck sequins', 'zipper kept', 'lace', 'wore', 'sequins getting', 'back', 'get', 'get caught', 'night']\n",
    "- Topic #18:\n",
    "['low', 'cut', 'low cut', 'little low', 'front', 'pin', 'safety', 'cut front', 'safety pin', 'cut chest', 'chest', 'back low', 'cut really', 'neck', 'pretty low', 'taste', 'fixed', 'cleavage', 'cut low', 'front cut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_topics = pd.DataFrame(topic_vector)\n",
    "# remove bad topics \n",
    "good_topics = {1, 2, 4, 5, 8, 18}\n",
    "bad_topics = set(range(20)) - good_topics\n",
    "for topic in bad_topics:\n",
    "    del df_topics[topic]\n",
    "# rename to understandable column names \n",
    "df_topics.columns = ['zipper', 'too_long', 'bra_prob', 'sequins_prob', 'too_short', 'low_cut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# associate the topic probabilities to the sentences\n",
    "df_problem_sent = df_problem_sent.reset_index().join(df_topics)\n",
    "df_problem_sent = df_problem_sent.join(df_dress, on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group problems by dress\n",
    "df_problems = df_problem_sent.groupby('url').mean()\n",
    "del df_problems['index']\n",
    "del df_problems['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "df_norm = pd.DataFrame(normalize(df_problems, axis=0, norm='max'), columns = df_problems.columns)\n",
    "df_norm.index = df_problems.index\n",
    "df_norm.to_csv('../data/dress_problems.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id=\"negBt\"></a>\n",
    "### Split negative topics per body type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_body = pd.read_csv('../data/measurement_data.csv', index_col=0)\n",
    "df_body_problems = df_problem_sent.join(df_body, on='index').dropna()\n",
    "body_topics = ['too_long', 'too_short', 'low_cut'] # topics that are body type related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_body_problems = df_body_problems[['url', 'kmean_label_2'] + body_topics]\n",
    "# group by dress and cluster\n",
    "df_body_problems = df_body_problems.groupby(['url', 'kmean_label_2'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_body_problems['total_prob'] = df_body_problems.iloc[:, 2:].sum(axis=1)\n",
    "df_body_problems = df_body_problems.pivot('kmean_label_2', 'url', 'total_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill nans with 0s \n",
    "df_body_problems = df_body_problems.fillna(0)\n",
    "df_body_problems = pd.DataFrame(normalize(df_body_problems, axis=0, norm='max'), columns= df_body_problems.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_body_problems.to_csv('../data/dress_features_bt.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
