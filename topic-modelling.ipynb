{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "- [helper functions](#helper)\n",
    "\n",
    "## [Tokenizing](#tokenization)\n",
    "- [sentence tokenization](#sentenceTokenization)\n",
    "- [cleaning](#cleaning)\n",
    "\n",
    "## [NMF](#nmf1)\n",
    "- [5 topics](#nmf1-topic5)\n",
    "    - [topic analysis](#nmf1-topic5-analysis)\n",
    "- [9 topics](#nmf1-topic9)\n",
    "    - [topic analysis](#nmf1-topic9-analysis)\n",
    "\n",
    "## [NMF With Only Nouns](#nmf2)\n",
    "- [extract nouns with NLTK](#extractNouns1)\n",
    "    - [max df = 0.01](#max1)\n",
    "        - [5 topics](#max1-topic5)\n",
    "            - [topic analysis](#nmf2-topic5-analysis)\n",
    "        - [9 topics](#max1-topic9)\n",
    "            - [topic analysis](#max1-topic9-analysis)\n",
    "    - [max df = 0.3](#max2)\n",
    "        - [5 topics](#max2-topic5)\n",
    "            - [topic analysis](#max2-topic5-analysis)\n",
    "        - [9 topics](#max2-topic9)\n",
    "            - [topic analysis](#max2-topic9-analysis)\n",
    "- [extract nouns with TextBlob](#extractNouns2)\n",
    "    - [topic analysis](#nmf3-topic5-analysis)\n",
    "    \n",
    "## [Model Review](#modelReview)\n",
    "\n",
    "## [Evaluating Likeness](#likeness)\n",
    "\n",
    "## [Feature Engineering Topics](#featureEngineeringTopics)\n",
    "- [Sentiment analysis](#sentimentAnalysis)\n",
    "- [Topic probability analysis](#topicProbability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import shuffle\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seaborn import plt\n",
    "import matplotlib.pyplot as mplt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import time\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('ec2-34-198-179-91.compute-1.amazonaws.com', 27017)\n",
    "db = client.fletcher\n",
    "dress_col = db.rtr_dresses\n",
    "rev_col = db.rtr_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur = rev_col.find({}, {\"review\":1, \"title\":1,\"_id\":0})\n",
    "rev_df = pd.DataFrame(list(cur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"helper\"></a>\n",
    "### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_id, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}\".format(topic_id))\n",
    "        words = [feature_names[i].strip() for i, v in (sorted(enumerate(topic), key=lambda x:x[1], reverse=True)[:n_top_words])]\n",
    "        print(', '.join(words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tfidf_and_tf(text, stopwords, max_df=0.90, min_df=0.001, ngram=(2,2), vocab=None):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                       ngram_range=ngram,\n",
    "                                       stop_words=sw, vocabulary = vocab)\n",
    "    t0 = time.time()\n",
    "    tfidf = tfidf_vectorizer.fit_transform(text)\n",
    "    print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "\n",
    "    # Use tf (raw term count) features for LDA.\n",
    "    print(\"Extracting tf features for LDA...\")\n",
    "    tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                    ngram_range=ngram,\n",
    "                                    stop_words=sw)\n",
    "    t0 = time.time()\n",
    "    tf = tf_vectorizer.fit_transform(text)\n",
    "    print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "    return tfidf, tfidf_vectorizer, tf, tf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tokenization\"></a>\n",
    "## Tokenization\n",
    "<a id=\"sentenceTokenization\"></a>\n",
    "### Sentence Tokenization\n",
    "It seems like there are multiple topics per review. Users comment on fit, occasion, recommendations, use of undergarments, and overall impression of the dress (beautiful, sparkly, etc). I will separate each comment to sentences using sent_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = rev_df.review.apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sent = pd.concat([pd.DataFrame({'review': x, 'index': i}) for i,x in enumerate(sentences)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cleaning\"></a>\n",
    "### Cleaning\n",
    "Get rid of punctuation, capital letters, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sent.review = df_sent.review.str.replace(r'[\\.\\,]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sent.review = df_sent.review.str.replace('-', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nmf1\"></a>\n",
    "## NMF\n",
    "- max distribution frequency = 0.05\n",
    "- (1,2) ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 7.682s.\n",
      "Extracting tf features for LDA...\n",
      "done in 6.799s.\n"
     ]
    }
   ],
   "source": [
    "tfidf, tfidf_vectorizer, tf, tf_vectorizer = get_tfidf_and_tf(df_sent.review, sw, min_df=0, max_df=0.05, ngram=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nmf1-topic5\"></a>\n",
    "### Extracting 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 29.800s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "loved, loved dress, absolutely loved, absolutely, overall loved, overall, everyone loved, loved loved, everyone, loved fit, loved wearing, really loved, much, loved pockets, dress much, pockets, dress got, dress would, wearing, loved everything\n",
      "Topic #1:\n",
      "many compliments, many, received, received many, got, got many, compliments dress, night, compliments night, received compliments, felt, got compliments, beautiful, night long, tons, tons compliments, long, throughout, compliments throughout, compliments felt\n",
      "Topic #2:\n",
      "true, true size, fit true, fits true, fits, dress true, runs true, runs, dress fit, size comfortable, dress fits, ran true, pretty true, ran, dress runs, pretty, flattering, size flattering, comfortable true, length\n",
      "Topic #3:\n",
      "rent, definitely, would definitely, definitely rent, recommend, highly, highly recommend, definitely recommend, recommend dress, would rent, rent dress, would highly, would recommend, rent runway, runway, dress would, rtr, overall, renting, anyone\n",
      "Topic #4:\n",
      "fit perfectly, perfectly, dress fit, glove, like glove, fit like, like, fit great, size fit, well, fit well, usually, perfectly comfortable, heels, length, usually wear, ordered, inch, felt, beautiful\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_topics = 5\n",
    "t0 = time.time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nmf1-topic5-analysis\"></a>\n",
    "#### Topic Analysis\n",
    "1. Topic 0 = Loved the dress\n",
    "2. Topic 1 = Received a lot of compliments\n",
    "3. Topic 2 = Good fit, true to size.\n",
    "4. Topic 3 = Would definitely rent again or recommend.\n",
    "5. Topic 4 = Dress was beautiful.\n",
    "\n",
    "This is a good start to the topic analysis. I can certainly use these topics to measure how much a user likes the dress.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nmf1-topic9\"></a>\n",
    "### Extracting 9 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 56.028s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "loved, loved dress, absolutely loved, absolutely, overall loved, overall, everyone loved, loved loved, everyone, loved fit, loved wearing, really loved, much, loved pockets, dress much, pockets, dress got, dress would, wearing, loved everything\n",
      "Topic #1:\n",
      "many compliments, many, received, received many, got, got many, compliments dress, night, compliments night, received compliments, got compliments, felt, night long, tons, tons compliments, throughout, compliments throughout, compliments felt, dress received, throughout night\n",
      "Topic #2:\n",
      "true, true size, fit true, fits true, fits, dress true, runs true, runs, dress fit, size comfortable, dress fits, ran true, pretty true, ran, dress runs, pretty, flattering, size flattering, comfortable true, size fit\n",
      "Topic #3:\n",
      "rent, definitely, definitely rent, would definitely, would rent, rent dress, definitely recommend, rent runway, runway, rtr, dress would, recommend, rent rtr, definitely wear, first, experience, wait rent, wait, rent one, renting\n",
      "Topic #4:\n",
      "fit perfectly, perfectly, dress fit, fit great, size fit, perfectly comfortable, usually, well, fit well, usually wear, worked perfectly, 10, ordered, small, small fit, worked, fits perfectly, wear size, wear fit, 10 fit\n",
      "Topic #5:\n",
      "recommend, highly, highly recommend, recommend dress, definitely recommend, would highly, would recommend, definitely, would definitely, anyone, dress anyone, recommend renting, overall, recommend anyone, renting, dress highly, dress would, formal, event, highly recommended\n",
      "Topic #6:\n",
      "beautiful, beautiful dress, dress beautiful, felt, felt beautiful, color, absolutely beautiful, absolutely, beautiful comfortable, overall, color beautiful, overall beautiful, beautiful fit, beautiful person, person, comfortable beautiful, dress absolutely, beautiful color, really beautiful, flattering\n",
      "Topic #7:\n",
      "glove, like glove, like, fit like, dress fit, felt like, fits like, felt, fits, fit great, dress fits, like million, million, feel, made, feel like, glove comfortable, million bucks, bucks, star\n",
      "Topic #8:\n",
      "length, heels, length perfect, inch, inch heels, long, perfect length, wore inch, short, regular, wore heels, heels length, perfect heels, night, regular length, perfect inch, heels perfect, heels dress, floor, compliments night\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_topics = 9\n",
    "t0 = time.time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nmf1-topic9-analysis\"></a>\n",
    "#### Topic breakdown\n",
    "1. Topic 0 = Loved the dress. Pockets Makes people happy.\n",
    "2. Topic 1 = Received a lot of compliments\n",
    "3. Topic 2 = Good fit, true to size. Flattering fit.\n",
    "4. Topic 3 = Would definitely rent again or recommend.\n",
    "5. Topic 4 = Dress was beautiful.\n",
    "6. Topic 5 = Recommendation\n",
    "7. Topic 6 = Beautiful\n",
    "8. Topic 7 = Fits like a glove.\n",
    "9. Topic 8 = Dress Length\n",
    "\n",
    "** 5 is the better topic number. Topics 5 - 8 seems to be repeating itself. ** I should also get rid of adjectives, keep only nouns, because I am getting how a user feels about the dress instead of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nmf2\"> </a>\n",
    "## NMF With Nouns\n",
    "<a id=\"extractNouns1\"> </a>\n",
    "## Extract nouns with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sent.review = df_sent.review.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_noun(s):\n",
    "    return [x[0] for x in nltk.pos_tag(s) if x[1] == 'NN' or x[1] == 'NNS']\n",
    "\n",
    "# nouns = pd.read_csv('data/review_nouns.csv')\n",
    "nouns = df_sent.review.str.split().apply(is_noun)\n",
    "nouns = nouns.str.join(' ')\n",
    "# saving nouns \n",
    "# nouns.to_csv('data/review_nouns.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"max1\"> </a>\n",
    "## Max DF = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.348s.\n",
      "Extracting tf features for LDA...\n",
      "done in 2.455s.\n"
     ]
    }
   ],
   "source": [
    "tfidf, tfidf_vectorizer, tf, tf_vectorizer = get_tfidf_and_tf(nouns, sw, max_df=0.01, ngram=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"max1-topic5\"> </a>\n",
    "### 5 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 6.416s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "dress size, size size, size dress, size fit, backup, order, reviews dress, fit dress, backup size, size length, size backup, stretch, order size, backup dress, case, sizes, size bit, medium, back dress, dress dress\n",
      "Topic #1:\n",
      "sleeves, shoulders, winter, thing, issue, problem, neckline, lace, look, snug, itchy, shoulder, complaint, tight, nothing, part, stretch, weather, straps, one\n",
      "Topic #2:\n",
      "compliments dress, tons, tons compliments, lots, lots compliments, dress night, lot compliments, ton, ton compliments, people, evening, complements, strangers, ball, dress wedding, people dress, prom, friends, everyone, fun\n",
      "Topic #3:\n",
      "glove, fit glove, dress glove, fits, size fit, curves, stretch, places, fits size, medium, room, snug, flattering, length heels, backup, everything, spanx, shape, beautiful, gown\n",
      "Topic #4:\n",
      "chest, top, shoulders, tape, room, front, medium, snug, cleavage, bottom, tight, someone, fashion, problem, fashion tape, straps, lace, girls, stretch, side\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_topics = 5\n",
    "t0 = time.time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"max1-topic5-analysis\"> </a>\n",
    "#### Topic Analysis\n",
    "This selection of topics is not as clear cut as the previous models. There are a lot of repeats between topic 0, 4 and 5. \n",
    "1. Topic 0: Size\n",
    "2. Topic 1: Shoulders, sleeves (winter = long sleeves?), upper part of the dress \n",
    "3. Topic 2: Compliments \n",
    "4. Topic 3: Fits well\n",
    "5. Topic 4: Fitting issues (room, snug, cleavage, tight, tapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"max1-topic9\"> </a>\n",
    "### 9 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 11.109s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "dress size, size size, size dress, size fit, order, backup, reviews dress, fit dress, backup size, stretch, size length, size backup, order size, backup dress, case, sizes, size bit, medium, back dress, dress dress\n",
      "Topic #1:\n",
      "compliments dress, tons, tons compliments, lots, lots compliments, dress night, lot compliments, ton, ton compliments, people, evening, complements, strangers, dress wedding, ball, people dress, prom, friends, everyone, dress color\n",
      "Topic #2:\n",
      "sleeves, shoulders, winter, thing, issue, problem, neckline, lace, snug, itchy, look, shoulder, tight, complaint, nothing, stretch, part, weather, medium, straps\n",
      "Topic #3:\n",
      "glove, fit glove, dress glove, fits, size fit, curves, stretch, places, fits size, medium, room, snug, flattering, length heels, backup, everything, spanx, shape, beautiful, dress curves\n",
      "Topic #4:\n",
      "chest, shoulders, room, tape, snug, medium, tight, front, cleavage, someone, fashion, fashion tape, problem, cup, issue, safety, straps, girls, stomach, stretch\n",
      "Topic #5:\n",
      "pockets, phone, skirt, part, plus, fun, clutch, fact, shape, detail, picture, everything, thing, pictures, great, room, comfortable, need, bag, love\n",
      "Topic #6:\n",
      "gold, shoes, person, dress gold, earrings, jewelry, pictures, picture, accessories, look, necklace, bottom, silver, photos, clutch, detail, statement, photo, dress person, fun\n",
      "Topic #7:\n",
      "gown, ball, floor, tie, people, tie wedding, one, gala, choice, evening, style, dress tie, everyone, order, lace, affair, experience, women, prom, husband\n",
      "Topic #8:\n",
      "top, bottom, tape, top dress, lace, look, front, straps, fashion, fashion tape, girls, shoulders, problem, shape, skirt, snug, cleavage, room, safety, issue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_topics = 9\n",
    "t0 = time.time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"max1-topic9-analysis\"> </a>\n",
    "#### Topic Analysis\n",
    "\n",
    "1. Topic 0: Size\n",
    "2. Topic 1: Shoulders, sleeves (winter = long sleeves?), upper part of the dress \n",
    "3. Topic 2: Compliments \n",
    "4. Topic 3: Fits well\n",
    "5. Topic 4: Fitting issues (room, snug, cleavage, tight, tapes)\n",
    "6. Topic 5: Phone (pockets/ clutch)\n",
    "7. Topic 6: Accessories (gold/silver)\n",
    "8. Topic 7: Event type (wedding, black tie)\n",
    "9. Topic 8: Fashion tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"max2\"> </a>\n",
    "## Max DF = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.447s.\n",
      "Extracting tf features for LDA...\n",
      "done in 2.157s.\n"
     ]
    }
   ],
   "source": [
    "tfidf, tfidf_vectorizer, tf, tf_vectorizer = get_tfidf_and_tf(nouns, sw, max_df=0.3, ngram=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"max2-topic5\"> </a>\n",
    "### 5 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.275s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "size, dress size, size size, fit size, size dress, size fit, reviews, backup, order, bust, dresses, bit, back, backup size, order size, waist, chest, size backup, area, sizes\n",
      "Topic #1:\n",
      "fit, dress fit, fit size, glove, fit glove, size fit, fit dress, bit, bust, waist, fit perfect, perfect, color, area, body, fit well, hips, medium, well, great\n",
      "Topic #2:\n",
      "compliments, night, compliments night, compliments dress, tons, tons compliments, dress compliments, lots, lots compliments, lot, ton, ton compliments, lot compliments, wedding, dress night, night dress, people, event, party, evening\n",
      "Topic #3:\n",
      "heels, length, inch, inch heels, length heels, heels length, bit, floor, heels dress, length inch, dress length, flats, ground, height, heel, length dress, dress heels, shoes, perfect, inches\n",
      "Topic #4:\n",
      "bra, back, bit, strapless, bra dress, bust, strapless bra, dress bra, cut, straps, top, chest, backless, tape, front, spanx, need, backless bra, area, fabric\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_topics = 5\n",
    "t0 = time.time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"max2-topic5-analysis\"> </a>\n",
    "#### Topic Analysis\n",
    "This maximum frequency works a lot better for nouns because we're already cutting a lot of the unnecessary words. \n",
    "\n",
    "1. Topic 0: Size\n",
    "2. Topic 1: Fit\n",
    "3. Topic 2: Compliments \n",
    "4. Topic 3: Heels/length\n",
    "5. Topic 4: Bra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"max2-topic9\"> </a>\n",
    "### 9 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 11.449s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "size, dress size, size size, fit size, size dress, size fit, backup, reviews, order, dresses, bust, backup size, back, order size, waist, size backup, chest, fits, sizes, fits size\n",
      "Topic #1:\n",
      "fit, dress fit, fit size, glove, fit glove, size fit, fit dress, bust, fit perfect, waist, perfect, fit well, body, well, medium, great, color, fit length, fit perfectly, area\n",
      "Topic #2:\n",
      "compliments, compliments dress, compliments night, tons, tons compliments, lots, lots compliments, dress compliments, lot compliments, ton compliments, ton, lot, evening, strangers, people, everyone, ball, party, women, gown\n",
      "Topic #3:\n",
      "heels, inch, inch heels, heels dress, floor, length heels, heels length, ground, length inch, dress heels, dress floor, height, inches, heels floor, flats, perfect, shoes, regular, order, taller\n",
      "Topic #4:\n",
      "bra, back, strapless, bra dress, strapless bra, bust, dress bra, straps, cut, backless, tape, top, chest, need, backless bra, spanx, front, strap, support, area\n",
      "Topic #5:\n",
      "night, compliments night, dress night, night dress, end night, end, sequins, people, night long, long, complements, arms, party, fun, vegas, date, material, girls, everyone, sleeves\n",
      "Topic #6:\n",
      "length, length heels, dress length, heels length, length dress, flats, heel, length inch, size length, fit length, knee, inch, floor, height, shoes, sleeves, reviews, issue, tall, gown\n",
      "Topic #7:\n",
      "bit, sequins, arms, dress bit, bust, zipper, waist, fabric, material, area, sleeves, top, back, hips, stretch, chest, sequins arms, shoulders, side, way\n",
      "Topic #8:\n",
      "wedding, event, tie, color, dress wedding, tie wedding, perfect, rtr, party, dress tie, material, dresses, fall, winter, time, day, wedding dress, fabric, gold, dress event\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_topics = 9\n",
    "t0 = time.time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"max2-topic9-analysis\"> </a>\n",
    "#### Topic Analysis\n",
    "\n",
    "1. Topic 0: Size\n",
    "2. Topic 1: Fit\n",
    "3. Topic 2: Compliments \n",
    "4. Topic 3: Heels/length\n",
    "5. Topic 4: Bra\n",
    "6. Topic 5: Night (Not very important)\n",
    "7. Topic 6: Length (repeat of topic 3)\n",
    "8. Topic 7: Details - sequins, zipper, fabric, material\n",
    "9. Topic 8: Occasion (wedding, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"extractNouns2\"></a>\n",
    "## Extract nouns with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def get_nouns(text):\n",
    "    return TextBlob(text).noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tb_nouns = pd.read_csv('data/tb_nouns.csv')\n",
    "tb_nouns = rev_df.review.apply(get_nouns)\n",
    "tb_nouns = tb_nouns.str.join(' ')\n",
    "# tb_nouns.to_csv('data/tb_nouns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.671s.\n",
      "Extracting tf features for LDA...\n",
      "done in 2.519s.\n"
     ]
    }
   ],
   "source": [
    "tfidf, tfidf_vectorizer, tf, tf_vectorizer = get_tfidf_and_tf(tb_nouns, sw, max_df=0.1, ngram=(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.178s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "inch heels, length inch heels, length inch, inch heels perfect, inch heels overall, heels perfect, heels overall, dress inch, dress inch heels, regular length, heels inch, size inch heels, size inch, inch heels perfect length, perfect length inch heels, perfect length inch, heels perfect length, heels inch heels, heels dress, heels rtr\n",
      "Topic #1:\n",
      "great dress, overall great dress, overall great, dress great, dress rtr, dress overall, dress formal, dress perfect, bachelorette party, heels great, dress good, area great, formal event, dress fit, party great, dress dress, bit snug, rent runway, holiday party, bust area\n",
      "Topic #2:\n",
      "black tie, tie wedding, black tie wedding, tie event, black tie event, tie optional, black tie optional, wore black tie, wore black, dress black tie, dress black, perfect black tie, perfect black, strapless bra, optional wedding, tie optional wedding, black tie optional wedding, dress black tie wedding, wore black tie wedding, black tie affair\n",
      "Topic #3:\n",
      "right places, hugs right places, hugs right, dress right, fit right, whole night, bachelorette party, stretchy material, rent runway, gorgeous dress, dress hugs, high heels, perfect dress, heels right, body types, thanks rtr, show stopper, perfect amount, thank rtr, holiday party\n",
      "Topic #4:\n",
      "perfect length, heels perfect, heels perfect length, perfect length inch, length inch, inch heels perfect length, inch heels perfect, perfect length inch heels, dress perfect, length inch heels, strapless bra, length perfect, size perfect, fit perfect, bra perfect, length overall, loved perfect, rtr perfect, tiny bit, rent runway\n",
      "Topic #5:\n",
      "beautiful dress, overall beautiful dress, overall beautiful, dress rtr, chest area, dress size, dress fit, dress beautiful, dress great, lace detail, bust area, rent runway, tiny bit, backup size, body type, regular length, dress loved, dress overall, long size, arm holes\n",
      "Topic #6:\n",
      "regular bra, safety pin, dress regular, strapless bra, rent runway, bra great, bra overall, backless bra, length regular, size regular, bra straps, gold color, heels regular, bra fashion, bra fashion tape, bra strap, large bust, bra perfect, backup dress, body types\n",
      "Topic #7:\n",
      "dress runs, chest area, gorgeous dress, bust area, backup size, regular length, dress dress, normal size, heels dress, strapless bra, safety pins, rib cage, size dress, party dress, rent runway, customer service, wedding dress, formal affair, broad shoulders, safety pin\n",
      "Topic #8:\n",
      "fashion tape, bust area, safety pin, chest area, tape overall, bra fashion tape, bra fashion, backup size, senior prom, strapless bra, rent runway, formal event, normal size, safety pins, normal bra, whole night, ca wait, perfect dress, perfect condition, holiday party\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_topics = 9\n",
    "t0 = time.time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nmf3-topic5-analysis\"> </a>\n",
    "#### Topic Analysis\n",
    "\n",
    "1. Topic 0: Beautiful dress\n",
    "2. Topic 1: Heels\n",
    "3. Topic 2: Bra\n",
    "4. Topic 3: Great overall\n",
    "5. Topic 4: Event\n",
    "\n",
    "The only good group of words I'm getting here is at topic 4, the sort of words describing the event is better TextBlob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modelReview\"></a>\n",
    "## Model Review\n",
    "I can group the topics to these categories:\n",
    "1. How much the person likes the dress\n",
    "    1. General like\n",
    "        - Beautifulness\n",
    "        - Recommend\n",
    "        - Received compliments\n",
    "    2. Like that can be linked to their body types\n",
    "        - Bra\n",
    "        - Heels usage\n",
    "        - Good fit, true to size\n",
    "    \n",
    "2. Dress attributes \n",
    "    - Use or bra\n",
    "    - Beautifulness\n",
    "    - Heels (high heels, inches, flats)\n",
    "    - Event it's used for\n",
    "    - Other features\n",
    "\n",
    "Therefore, I need these topics: \n",
    "- Beautiful\n",
    "- Recommend\n",
    "- Compliments\n",
    "- Bra Issues\n",
    "- Heels\n",
    "- Good fit\n",
    "- Event\n",
    "\n",
    "    \n",
    "From these things, we can recommend a dress given a certain body type.\n",
    "- We can access the general like using [this](#nmf1-topic5).\n",
    "- Looking at [this](#max1-topic9), I can also see that the more topic number you want, the more details you get. This is meaningful only when you're dealing with the nouns.\n",
    "\n",
    "<a id=\"likeness\"></a>\n",
    "## Evaluating Likeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 7.611s.\n",
      "Extracting tf features for LDA...\n",
      "done in 6.554s.\n"
     ]
    }
   ],
   "source": [
    "tfidf, tfidf_vectorizer, tf, tf_vectorizer = get_tfidf_and_tf(df_sent.review, sw, min_df=0, max_df=0.05, ngram=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "['loved', 'loved dress', 'absolutely loved', 'absolutely', 'overall loved']\n",
      "Topic 1\n",
      "['many compliments', 'many', 'received', 'received many', 'got']\n",
      "Topic 2\n",
      "['true', 'true size', 'fit true', 'fits true', 'fits']\n",
      "Topic 3\n",
      "['rent', 'definitely', 'would definitely', 'definitely rent', 'recommend']\n",
      "Topic 4\n",
      "['fit perfectly', 'perfectly', 'dress fit', 'glove', 'like glove']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=5, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_like = pd.DataFrame(nmf.transform(tfidf), columns=['love', 'compliment', 'true_fit', 'recommend', 'perfect_fit'])\n",
    "# true fit suggests that the dress came true to size\n",
    "# perfect fit suggests that the dress fits well for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_like['topic'] = df_like.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_like = df_sent.join(df_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>love</th>\n",
       "      <th>compliment</th>\n",
       "      <th>true_fit</th>\n",
       "      <th>recommend</th>\n",
       "      <th>perfect_fit</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fits true to size</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>true_fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i'm 145 lb 5'1\" and the 10r fit nice except it...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>perfect_fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>got compliments from wedding guests i didn't e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>compliment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i wish i could have gotten the 16l</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i am 5'9 and the 16r was a tad short with my h...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>perfect_fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>other than that i would recommend this dress f...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>i loved this dress so much!</td>\n",
       "      <td>0.044943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>i got lots of compliments and it was very comf...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>compliment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>i wore a 2r but next time i would definitely g...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>even with 3 inch heels i had to carry it every...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>perfect_fit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                             review      love  \\\n",
       "0      0                                  fits true to size  0.000000   \n",
       "1      0  i'm 145 lb 5'1\" and the 10r fit nice except it...  0.000000   \n",
       "2      0  got compliments from wedding guests i didn't e...  0.000000   \n",
       "3      1                 i wish i could have gotten the 16l  0.000000   \n",
       "4      1  i am 5'9 and the 16r was a tad short with my h...  0.000000   \n",
       "5      1  other than that i would recommend this dress f...  0.000000   \n",
       "6      2                        i loved this dress so much!  0.044943   \n",
       "7      2  i got lots of compliments and it was very comf...  0.000000   \n",
       "8      2  i wore a 2r but next time i would definitely g...  0.000000   \n",
       "9      2  even with 3 inch heels i had to carry it every...  0.000000   \n",
       "\n",
       "   compliment  true_fit  recommend  perfect_fit        topic  \n",
       "0    0.000000  0.065298   0.000000     0.000000     true_fit  \n",
       "1    0.000053  0.000000   0.000000     0.000578  perfect_fit  \n",
       "2    0.004855  0.000000   0.000000     0.000000   compliment  \n",
       "3    0.000000  0.000000   0.000000     0.000000         love  \n",
       "4    0.000000  0.000000   0.000000     0.000215  perfect_fit  \n",
       "5    0.000000  0.000000   0.006444     0.000000    recommend  \n",
       "6    0.000000  0.000000   0.000000     0.000000         love  \n",
       "7    0.008206  0.000000   0.000000     0.000000   compliment  \n",
       "8    0.000000  0.000000   0.011736     0.000000    recommend  \n",
       "9    0.000000  0.000000   0.000000     0.000729  perfect_fit  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_like.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"details\"></a>\n",
    "## Evaluating Dress Details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.277s.\n",
      "Extracting tf features for LDA...\n",
      "done in 2.495s.\n"
     ]
    }
   ],
   "source": [
    "tfidf, tfidf_vectorizer, tf, tf_vectorizer = get_tfidf_and_tf(nouns, sw, max_df=0.8, ngram=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "dress, dress fit, dress size, compliments dress, party, dress dress, dress night, dress compliments, size dress, ball\n",
      "Topic 1\n",
      "size, dress size, size size, fit size, size dress, size fit, backup, reviews, order, backup size\n",
      "Topic 2\n",
      "fit, dress fit, fit size, fit glove, glove, size fit, fit dress, fit perfect, perfect, fit well\n",
      "Topic 3\n",
      "compliments, compliments dress, compliments night, tons, tons compliments, lots compliments, lots, dress compliments, lot compliments, ton compliments\n",
      "Topic 4\n",
      "heels, inch, inch heels, heels dress, length heels, floor, heels length, ground, length inch, dress heels\n",
      "Topic 5\n",
      "bra, strapless, bra dress, strapless bra, dress bra, straps, backless, cut, tape, need\n",
      "Topic 6\n",
      "night, compliments night, dress night, night dress, end night, end, night long, people, long, complements\n",
      "Topic 7\n",
      "length, length heels, dress length, heels length, length dress, flats, heel, length inch, size length, fit length\n",
      "Topic 8\n",
      "bit, dress bit, sleeves, arms, top, area, size bit, chest, stretch, side\n",
      "Topic 9\n",
      "wedding, tie, tie wedding, dress wedding, perfect, dress tie, fall, winter, party, wedding dress\n",
      "Topic 10\n",
      "sequins, arms, sequins arms, hair, dress sequins, sleeves, arm, scratchy, skin, itchy\n",
      "Topic 11\n",
      "color, gold, dress color, person, cut, picture, shoes, style, skin, dress gold\n",
      "Topic 12\n",
      "rtr, time, experience, experience rtr, dress rtr, time rtr, rtr dress, rtr experience, order, service\n",
      "Topic 13\n",
      "material, dress material, stretch, quality, stretchy, body, curves, lot, places, winter\n",
      "Topic 14\n",
      "event, dress event, work, perfect, party, tie, event dress, tie event, time, day\n",
      "Topic 15\n",
      "back, back dress, front, dress back, train, cut, part, straps, lace, reviews\n",
      "Topic 16\n",
      "fabric, stretch, lot, stretchy, way, quality, skirt, hips, curves, give\n",
      "Topic 17\n",
      "bust, waist, hips, area, way, chest, room, top, cut, shoulders\n",
      "Topic 18\n",
      "dresses, designer, one, sizes, bridesmaids, bridesmaid, way, time, everyone, rent\n",
      "Topic 19\n",
      "zipper, way, side, help, issue, problem, someone, reviews, con, complaint\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=20, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0: Dress fit   \n",
    "Topic 1: Size  \n",
    "Topic 2: Dress fit  \n",
    "Topic 3: Compliments  \n",
    "Topic 4: Inches  \n",
    "heels, inch, inch heels, heels dress, length heels, floor, heels length, ground, length inch, dress heels  \n",
    "Topic 5: Bra  \n",
    "bra, strapless, bra dress, strapless bra, dress bra, straps, backless, cut, tape, need  \n",
    "Topic 6: Night  \n",
    "night, compliments night, dress night, night dress, end night, end, night long, people, long, complements  \n",
    "Topic 7: Length  \n",
    "length, length heels, dress length, heels length, length dress, flats, heel, length inch, size length, fit length  \n",
    "Topic 8: Arms/Sleeves  \n",
    "bit, dress bit, sleeves, arms, top, area, size bit, chest, stretch, side  \n",
    "Topic 9: Wedding  \n",
    "wedding, tie, tie wedding, dress wedding, perfect, dress tie, fall, winter, party, wedding dress  \n",
    "Topic 10: Sequins (itchy)  \n",
    "sequins, arms, sequins arms, hair, dress sequins, sleeves, arm, scratchy, skin, itchy  \n",
    "Topic 11: Color (gold)  \n",
    "color, gold, dress color, person, cut, picture, shoes, style, skin, dress gold  \n",
    "Topic 12: Experience  \n",
    "rtr, time, experience, experience rtr, dress rtr, time rtr, rtr dress, rtr experience, order, service\n",
    "Topic 13: Material\n",
    "material, dress material, stretch, quality, stretchy, body, curves, lot, places, winter\n",
    "Topic 14: Event  \n",
    "event, dress event, work, perfect, party, tie, event dress, tie event, time, day  \n",
    "Topic 15: Back  \n",
    "back, back dress, front, dress back, train, cut, part, straps, lace, reviews    \n",
    "Topic 16: Stretch  \n",
    "fabric, stretch, lot, stretchy, way, quality, skirt, hips, curves, give  \n",
    "Topic 17: Hips area  \n",
    "bust, waist, hips, area, way, chest, room, top, cut, shoulders  \n",
    "Topic 18: Not sure  \n",
    "dresses, designer, one, sizes, bridesmaids, bridesmaid, way, time, everyone, rent  \n",
    "Topic 19: Issues  \n",
    "zipper, way, side, help, issue, problem, someone, reviews, con, complaint  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = ['dress_fit', 'size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_details = pd.DataFrame(nmf.transform(tfidf), columns=['topic_{}'.format(x) for x in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drop = [0, 6, 7, 12, 18]\n",
    "for d in drop:\n",
    "    df_details = df_details.drop('topic_{}'.format(d), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_details.columns = ['size', 'dress_fit', 'compliments', 'length', 'bra', 'sleeves',\n",
    "                      'wedding', 'sequins', 'color', 'material', 'event', 'back',\n",
    "                     'stretch', 'hips', 'issues']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_details['topic'] = df_details.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_details = df_sent.join(df_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur = rev_col.find({}, {\"url\": 1, \"_id\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_urls = pd.DataFrame(list(cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_details = df_details.join(df_urls, on=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dresses = pd.DataFrame(list(dress_col.find({}, {\"_id\": 0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'review', 'size', 'dress_fit', 'compliments', 'length', 'bra',\n",
       "       'sleeves', 'wedding', 'sequins', 'color', 'material', 'event', 'back',\n",
       "       'stretch', 'hips', 'issues', 'topic', 'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_details.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"featureEngineeringTopics\"></a>\n",
    "\n",
    "## Feature Engineering Topics\n",
    "There are some topics where we must assess the polarity to see if it's rated positively/negatively (e.g. length, stretch).\n",
    "But there are also ones that can just be assessed by  just the fact that the topic is mentioned (e.g. back, sequins). The approach to measure these two categories is different: with the polarized one, we take the mean of the sentiment. With the latter one, we take the mean of the topic probability.\n",
    "### Polarized\n",
    "['size', 'dress_fit','length', 'bra', 'sleeves', 'sequins', 'color', 'material', 'back', 'stretch', 'hips', 'issues']\n",
    "### Positive\n",
    "['compliments', 'issues', 'sequins'] \n",
    "\n",
    "I've also identified a third type of topic: that is a categorical one. These topics can be assessed further through more topic analysis, to add more categories. (e.g length = is it short or long?)\n",
    "### Categorical\n",
    "['length', 'bra','wedding', 'event', 'back', 'issues']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sentimentAnalysis\"></a>\n",
    "### Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentiment(s):\n",
    "    return TextBlob(s).sentiment[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiments = df_details.review.apply(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_details['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_polarized = df_details[df_details.topic.isin(['size', 'dress_fit','length', \n",
    "                                                 'bra', 'sleeves', 'sequins', \n",
    "                                                 'color', 'material', 'back', \n",
    "                                                 'stretch', 'hips', 'issues'])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_polarized = pd.DataFrame(df_polarized.groupby(['url', 'topic'], as_index=False)['sentiment'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_polarized = df_polarized.pivot('url', 'topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"12\" halign=\"left\">sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th>back</th>\n",
       "      <th>bra</th>\n",
       "      <th>color</th>\n",
       "      <th>dress_fit</th>\n",
       "      <th>hips</th>\n",
       "      <th>issues</th>\n",
       "      <th>length</th>\n",
       "      <th>material</th>\n",
       "      <th>sequins</th>\n",
       "      <th>size</th>\n",
       "      <th>sleeves</th>\n",
       "      <th>stretch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://www.renttherunway.com/shop/designers/alexis/red_leona_dress</th>\n",
       "      <td>0.203861</td>\n",
       "      <td>0.096007</td>\n",
       "      <td>0.407753</td>\n",
       "      <td>0.423304</td>\n",
       "      <td>0.124177</td>\n",
       "      <td>0.081419</td>\n",
       "      <td>0.054844</td>\n",
       "      <td>0.290147</td>\n",
       "      <td>0.066022</td>\n",
       "      <td>0.279006</td>\n",
       "      <td>0.067228</td>\n",
       "      <td>0.105407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.renttherunway.com/shop/designers/allison_parris/cobalt_marilyn_gown</th>\n",
       "      <td>0.144366</td>\n",
       "      <td>0.140034</td>\n",
       "      <td>0.411457</td>\n",
       "      <td>0.360768</td>\n",
       "      <td>0.122181</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>0.317536</td>\n",
       "      <td>0.300392</td>\n",
       "      <td>0.157278</td>\n",
       "      <td>0.296506</td>\n",
       "      <td>0.061319</td>\n",
       "      <td>0.277693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.renttherunway.com/shop/designers/badgley_mischka/award_winner_gown</th>\n",
       "      <td>0.160555</td>\n",
       "      <td>0.140960</td>\n",
       "      <td>0.367764</td>\n",
       "      <td>0.467023</td>\n",
       "      <td>0.179997</td>\n",
       "      <td>0.152752</td>\n",
       "      <td>0.242220</td>\n",
       "      <td>0.313751</td>\n",
       "      <td>0.080275</td>\n",
       "      <td>0.286641</td>\n",
       "      <td>0.148834</td>\n",
       "      <td>0.243559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   sentiment            \\\n",
       "topic                                                   back       bra   \n",
       "url                                                                      \n",
       "https://www.renttherunway.com/shop/designers/al...  0.203861  0.096007   \n",
       "https://www.renttherunway.com/shop/designers/al...  0.144366  0.140034   \n",
       "https://www.renttherunway.com/shop/designers/ba...  0.160555  0.140960   \n",
       "\n",
       "                                                                        \\\n",
       "topic                                                  color dress_fit   \n",
       "url                                                                      \n",
       "https://www.renttherunway.com/shop/designers/al...  0.407753  0.423304   \n",
       "https://www.renttherunway.com/shop/designers/al...  0.411457  0.360768   \n",
       "https://www.renttherunway.com/shop/designers/ba...  0.367764  0.467023   \n",
       "\n",
       "                                                                        \\\n",
       "topic                                                   hips    issues   \n",
       "url                                                                      \n",
       "https://www.renttherunway.com/shop/designers/al...  0.124177  0.081419   \n",
       "https://www.renttherunway.com/shop/designers/al...  0.122181  0.142913   \n",
       "https://www.renttherunway.com/shop/designers/ba...  0.179997  0.152752   \n",
       "\n",
       "                                                                        \\\n",
       "topic                                                 length  material   \n",
       "url                                                                      \n",
       "https://www.renttherunway.com/shop/designers/al...  0.054844  0.290147   \n",
       "https://www.renttherunway.com/shop/designers/al...  0.317536  0.300392   \n",
       "https://www.renttherunway.com/shop/designers/ba...  0.242220  0.313751   \n",
       "\n",
       "                                                                        \\\n",
       "topic                                                sequins      size   \n",
       "url                                                                      \n",
       "https://www.renttherunway.com/shop/designers/al...  0.066022  0.279006   \n",
       "https://www.renttherunway.com/shop/designers/al...  0.157278  0.296506   \n",
       "https://www.renttherunway.com/shop/designers/ba...  0.080275  0.286641   \n",
       "\n",
       "                                                                        \n",
       "topic                                                sleeves   stretch  \n",
       "url                                                                     \n",
       "https://www.renttherunway.com/shop/designers/al...  0.067228  0.105407  \n",
       "https://www.renttherunway.com/shop/designers/al...  0.061319  0.277693  \n",
       "https://www.renttherunway.com/shop/designers/ba...  0.148834  0.243559  "
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_polarized.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_polarized = df_polarized.replace(np.nan, 0) # if it's not mentioned - it's neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topicProbability\"></a>\n",
    "### Topic probability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_prob = ['compliments', 'issues', 'sequins']\n",
    "df_topic_probability = df_details.groupby('index', as_index=False)[topic_prob].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_index_url = df_details.groupby('index')['index','url'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_topic_probability = df_topic_probability.merge(df_index_url, on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_topic_probability = df_topic_probability.groupby('url')[topic_prob].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With compliment and issues, it will be multiplied to the end result. It will give a weighting to each of the dress. Therefore, dress with more issues will be ranked relatively lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephinetirtanata/anaconda/lib/python3.5/site-packages/pandas/tools/merge.py:205: UserWarning: merging between different levels can give an unintended result (2 levels on the left, 1 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "df_complete = df_polarized.join(df_topic_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving everything to combine it with body type information.\n",
    "for each body type:  \n",
    "evaluate sentiment on fit, length, hips, size, sleeves and stretch depends on body types.  \n",
    "Thus, we have a dress space for each cluster.  \n",
    "Bra, sequins, color, material, back - this is quite general. These values remain the same in all of the clusters. \n",
    "Comfort depends on the fit and stretch for each body types.\n",
    "\n",
    "Recommendation\n",
    "1. See which cluster the person belongs to.\n",
    "2. Take their priorities.\n",
    "3. Take the closest N neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
